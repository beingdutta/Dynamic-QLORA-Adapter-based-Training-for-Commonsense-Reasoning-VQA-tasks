{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2a74c8-02f4-4419-a002-5f8920beae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import datetime\n",
    "from torch import nn\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dea976-86a2-475c-9ed2-1600a1d02d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from peft import LoraConfig\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef89e52-8079-43d4-a5a6-3ec6a6d16255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f42528-8bb6-47d3-9d88-270edc8d3c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "from util.vision_util import process_vision_info\n",
    "from util.logutil import init_logger, get_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf53e59-e0f8-414e-881d-9b6d444ba6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd66d7a-1db0-4d9a-bf17-4fc06fb9bdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed86902c-45d2-4e50-94fc-91b4e2e73deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de9d9b91-a33d-4fec-b35e-4657826868e9",
   "metadata": {},
   "source": [
    "### Dataset Loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d387c2db-0d55-4eb8-93cb-f35cf98d9838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UnPickling (DeSerialization)\n",
    "\n",
    "with open('/home/aritrad/moe-directory/moe-datasets/TDIUC/prototype-train-set-8k-for-router-machine-automatic-llama3.2-annotation.pickle', 'rb') as file:\n",
    "    mixed_reasoning_data_prototype = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f559a41-840f-4eee-b452-39bf01d2738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_reasoning_data_prototype[0:2], len(mixed_reasoning_data_prototype)\n",
    "# Count reasoning types\n",
    "reasoning_counts = Counter(item[\"reasoning_type\"] for item in mixed_reasoning_data_prototype)\n",
    "reasoning_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2a5842-313e-4cea-b323-5dc956a56f25",
   "metadata": {},
   "source": [
    "### Subsettting Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef1ec62-3e5d-4622-b7bf-043db0f5bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Subsetting for testing\n",
    "\n",
    "mixed_reasoning_data_prototype = mixed_reasoning_data_prototype[:500]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275fce5b-1a90-48b0-a55c-fa4b12db25c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Length of mixed reasoing dataset: {len(mixed_reasoning_data_prototype)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caa1d29-9a10-4dfe-a2b0-afdda2af5455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e3261-a8b6-4473-b2b7-d768306590e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder_path = \"/home/aritrad/moe-directory/moe-datasets/TDIUC/TDIUC/Images/val2014\"\n",
    "prefix = \"Generate a one word answer for the given image and question: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acde344-8f4e-4896-a7cb-06ee8001dbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_names = [\"Physical Reasoning.\", \"Quantity Reasoning.\", \"Spatial Reasoning.\", \"Social and Emotional Reasoning.\"]\n",
    "label2id = {name: idx for idx, name in enumerate(expert_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3836092b-26bf-4802-af26-e898b158d74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using list comprehension to update reasoning_type.\n",
    "\n",
    "mixed_reasoning_data_prototype = [\n",
    "    {**item, 'reasoning_type': label2id[item['reasoning_type']]}\n",
    "    for item in mixed_reasoning_data_prototype\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676c443c-8016-44f7-a855-c593099edc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_reasoning_data_prototype[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62896b8-410f-48a4-8ee5-70a21f215c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "listToDictionary = {\n",
    "    'question': [ prefix + dict_['question'] for dict_ in mixed_reasoning_data_prototype ], \n",
    "    'image': [ os.path.join(image_folder_path, dict_['image_id']) for dict_ in mixed_reasoning_data_prototype ],\n",
    "    'answer': [ dict_['answer'] for dict_ in mixed_reasoning_data_prototype ], \n",
    "    'expert_labels': [ dict_['reasoning_type'] for dict_ in mixed_reasoning_data_prototype ], \n",
    "}\n",
    "\n",
    "mixed_reasoning_data = Dataset.from_dict(listToDictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f758475-7b02-4ca2-a5e9-f7bf8b2d1b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mixed_reasoning_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd6b658-0c43-43df-8a9f-15c7c75ac732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into Train and Val Set\n",
    "\n",
    "split = mixed_reasoning_data.train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb19489-d7ec-4421-bf8f-6040c7df36ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = split['train']\n",
    "val_set = split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1685976c-e39f-4901-9b91-1b36b3535468",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set), len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8581187-a136-4eab-8d85-6999f8adb302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de354632-58b2-4342-a51d-2944fd6b17e9",
   "metadata": {},
   "source": [
    "### Creating JSON of the Qwen Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae41671-7e45-4c02-873f-4f703679d1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produceFormattedJSON(targetSet):\n",
    "    \n",
    "    formattedJSON = list()\n",
    "\n",
    "    for idx in tqdm(range(len(targetSet))):\n",
    "        currentJSON =   {\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"image\",\n",
    "                                \"image\": f\"{targetSet[idx]['image']}\"\n",
    "                            },\n",
    "                            {\"type\": \"text\", \"text\": f\"{targetSet[idx]['question']}\"}\n",
    "                        ]\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": f\"{targetSet[idx]['answer']}\"}\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            }  \n",
    "        formattedJSON.append(currentJSON)\n",
    "        \n",
    "    return formattedJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b29c481-387c-4a26-83b2-2b25f12350df",
   "metadata": {},
   "outputs": [],
   "source": [
    "formattedJSONTrain = produceFormattedJSON(train_set)\n",
    "formattedJSONVal = produceFormattedJSON(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e4be12-e7de-4388-9b8e-b55fe465d44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a JSON file\n",
    "output_file_train = \"/home/aritrad/moe-directory/moe-datasets/TDIUC/trash/mixed-train.json\"\n",
    "output_file_val = \"/home/aritrad/moe-directory/moe-datasets/TDIUC/trash/mixed-val.json\"\n",
    "\n",
    "# Use `indent` for pretty printing\n",
    "with open(output_file_train, \"w\") as file1, open(output_file_val, 'w') as file2:\n",
    "    json.dump(formattedJSONTrain, file1, indent=4)  \n",
    "    json.dump(formattedJSONVal, file2, indent=4)  \n",
    "\n",
    "print(f\"Data saved to: \\n{output_file_train} || {output_file_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5234c1-da12-4ee4-9650-93e1749e851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'train_output/{datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")}/'\n",
    "init_logger(output_dir)\n",
    "logger = get_logger()\n",
    "\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c75e56-14f2-4f9e-8db8-9195bf9b3d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46ab657c-cf2b-4610-a7b2-e84d88599756",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5441d31e-1e39-4a33-95e5-b6b1de6a4326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75912d14-0c2e-44c3-9454-be4441c91abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mixedTrainData(Dataset):\n",
    "    \n",
    "    def __init__(self, msg_path, orig_dataset):\n",
    "        with open(msg_path) as f:\n",
    "            self.msgs = json.load(f)\n",
    "        self.orig = orig_dataset   # HF Dataset with 'expert_labels'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.msgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.msgs[idx]\n",
    "        # copy the messages\n",
    "        out = {\"messages\": entry[\"messages\"]}\n",
    "        # pull the label from the original\n",
    "        out[\"expert_label\"] = self.orig[idx][\"expert_labels\"]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca712e0-9082-4e56-a37e-f713a7034f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = mixedTrainData(output_file_train, train_set)\n",
    "val_dataset = mixedTrainData(output_file_val, val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd49c7ac-8694-42f2-82f5-db70756f3d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c8a87f-f703-4902-b94b-a5aa89b055b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_collate(batch):\n",
    "    return {\n",
    "        \"messages\":     [ex[\"messages\"]     for ex in batch],\n",
    "        \"expert_label\": torch.tensor([ex[\"expert_label\"] for ex in batch], dtype=torch.long),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf1f703-c4ea-40d3-a7e2-aaee40d94f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7a96ca-cc40-4228-bd93-af5b8079d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = batch_,\n",
    "    shuffle = True,\n",
    "    collate_fn = raw_collate,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = batch_,\n",
    "    shuffle = True,\n",
    "    collate_fn = raw_collate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e4cf2-c08b-423e-a420-8559deac60fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Test\n",
    "\n",
    "for batch in val_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad3a20a-c8f6-414c-ab6a-d84ff6152b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total number of steps in: \\nTrain Loader: {len(train_loader)}\\nVal Loader: {len(val_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cb68d0-59b5-4ff6-8adb-1efe918a850b",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a6b791-c4cb-4b6d-956a-a9fa7b9e2f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_assistant_content_sublist_indexes(l):\n",
    "    '''\n",
    "    A message from train_data/data.json may look like below:\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {'role': 'user', 'content': [{'type': 'image', 'image': 'train_data/1.jpeg'}, {'type': 'text', 'text': '描述一下这个图片'}]}, \n",
    "                {'role': 'assistant', 'content': [{'type': 'text', 'text': '这张图片展示了一位年轻女子和她的狗在海滩上玩耍的场景。女子穿着格子衬衫和黑色裤子，坐在沙滩上，与她的金毛犬互动。她们的手臂伸展着，似乎在进行某种游戏或训练。背景是广阔的海洋和晴朗的天空，阳光洒在沙滩上，营造出温暖而宁静的氛围。整体画面充满了快乐和放松的感觉。'}]}\n",
    "            ]\n",
    "        }\n",
    "    After apply_chat_template, the text will look like below:\n",
    "        ['<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>描述一下这个图片<|im_end|>\\n<|im_start|>assistant\\n这张图片展示了一位年轻女子和她的狗在海滩上玩耍的场景。女子穿着格子衬衫和黑色裤子，坐在沙滩上，与她的金毛犬互动。她们的手臂伸展着，似乎在进行某种游戏或训练。背景是广阔的海洋和晴朗的天空，阳光洒在沙滩上，营造出温暖而宁静的氛围。整体画面充满了快乐和放松的感觉。<|im_end|>\\n']\n",
    "\n",
    "    This function tries to find the indexes of the assistant content in the input_ids list to build labels.\n",
    "    '''\n",
    "    start_indexes = []\n",
    "    end_indexes = []\n",
    "\n",
    "    # Iterate through the list to find starting points\n",
    "    for i in range(len(l) - 1):\n",
    "        # Check if the current and next elements form the start sequence\n",
    "        if l[i] == 151644 and l[i+1] == 77091 and l[i+2] == 198:\n",
    "            start_indexes.append(i+3)\n",
    "            # Now look for the first 151645 and 198 after the start\n",
    "            for j in range(i+3, len(l)-1):\n",
    "                if l[j] == 151645 and l[j+1] == 198:\n",
    "                    end_indexes.append(j+2) # **NOTE** the <|im_end|>\\n 2 tokens should be included in the label, so that model can predicate end of output.\n",
    "                    break  # Move to the next start after finding the end\n",
    "\n",
    "    return list(zip(start_indexes, end_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05f3fb9-3757-4fd7-9fa9-c4e1b0dd448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_router_inputs(batch):\n",
    "    \"\"\"\n",
    "    Extracts the question text from each example's `messages` and returns\n",
    "    SBERT embeddings of shape (B, 768) on `device`.\n",
    "    \n",
    "    batch: dict with key \"messages\": list[list[dict]]\n",
    "    \"\"\"\n",
    "    # For each example, the first message is the user prompt with content list\n",
    "    questions = [\n",
    "        next(item[\"text\"] for item in msgs[0][\"content\"] if item[\"type\"] == \"text\")\n",
    "        for msgs in batch[\"messages\"]\n",
    "    ]\n",
    "    # SBERT under no_grad, returns tensor on device\n",
    "    return sbert.encode(questions, convert_to_tensor=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c976baed-0f2c-4362-b2b1-b11f9975e7b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e4b0448-e933-4e90-821d-383f3e2f9028",
   "metadata": {},
   "source": [
    "### Define Router & Load Chkpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb0299-8a64-4eff-9c3b-bead524469cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Router(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden=768, n_experts=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden//2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden//2, hidden//4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden//4, n_experts)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764212a4-3260-41b1-afee-218541b2c6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for testing saved router checkpoint on other test sets.\n",
    "router = Router(n_experts=len(expert_names)).to(device)\n",
    "\n",
    "# SBERT for frozen text embeddings\n",
    "sbert = SentenceTransformer(\"all-mpnet-base-v2\", device=device)\n",
    "\n",
    "for p in sbert.parameters():\n",
    "    p.requires_grad_(False)\n",
    "    \n",
    "# Load the Router chceckpoint.\n",
    "checkpoint = torch.load('/home/aritrad/moe-directory/moe-datasets/TDIUC/custom-moe/using-sbert/router_best.pt') \n",
    "router.load_state_dict(checkpoint)\n",
    "\n",
    "print(\"Router Initialized ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d028421-26ed-449f-b5f7-743aa7e284bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af4c8179-17d6-4276-97fe-1b44008be4b2",
   "metadata": {},
   "source": [
    "### Qwen Backbone Model Loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbffad20-52eb-4ca6-8ff9-9546343d9913",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_use_double_quant = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dc42ec-6121-450f-8bfc-83c5be69ca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\", \n",
    "    attn_implementation = \"flash_attention_2\", \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=\"auto\", \n",
    "    quantization_config = bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de01959a-208e-452a-8f40-37d35f6cceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processor. \n",
    "# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", min_pixels=256*28*28, max_pixels=512*28*28, padding_side=\"left\", use_fast = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8af867a-1511-4a73-ade2-7744e1b98a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LORA Adapters will be trained as usual. (Only the Qwen Backbone will be freezed.)\n",
    "\n",
    "backbone.eval()\n",
    "for p in backbone.parameters(): \n",
    "    p.requires_grad_(False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772a0d75-fbb5-4596-adab-97a19d52a569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27e83785-499d-4cb4-ae29-2be48b3735b8",
   "metadata": {},
   "source": [
    "### Load Experts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac8f7d1-2c8a-401f-ac1f-5bfbcafc9532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load & **unfreeze** each LoRA adapter\n",
    "\n",
    "ADAPTER_ROOT = '/home/aritrad/moe-directory/moe-datasets/TDIUC/best_experts-r32-2.5/'\n",
    "\n",
    "expert_names = [\"physical\", \"quantitative\", \"spatial\", \"social\"]\n",
    "\n",
    "for name in expert_names:\n",
    "    path = os.path.join(ADAPTER_ROOT, name)\n",
    "    backbone.load_adapter(path, adapter_name=name, is_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d16e9a1-5c28-4728-915a-f76a72988862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "017d82ba-af77-46fa-bd3e-5332a89d12bb",
   "metadata": {},
   "source": [
    "### Hyperparams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e37af09-6b9c-439b-9902-1130734baee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils   import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854b9f0d-9683-4573-b07c-a1a9f886cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two‑group optimiser: router vs adapters\n",
    "EPOCHS       = 5\n",
    "router_lr   = 1e-5\n",
    "adapter_lr  = 5e-6               # lower step helps convergence\n",
    "weight_decay = 1e-2\n",
    "\n",
    "adapter_params = [p for p in backbone.parameters() if p.requires_grad]\n",
    "optimizer = AdamW(\n",
    "    [\n",
    "        {\"params\": router.parameters(),  \"lr\": router_lr},\n",
    "        {\"params\": adapter_params,       \"lr\": adapter_lr},\n",
    "    ],\n",
    "    weight_decay=weight_decay,\n",
    ")\n",
    "\n",
    "ce_router  = nn.CrossEntropyLoss()   # unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7340453-fc65-41d5-8bee-e8208904a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plateau scheduler configured to watch *adapter* group (index 1)\n",
    "def scheduler_step(val_loss):\n",
    "    plateau.step(val_loss)\n",
    "    optimizer.param_groups[0][\"lr\"] = 1e-5\n",
    "\n",
    "\n",
    "plateau = ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=1, min_lr=5e-7, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccac4f4c-97ff-4e60-8686-134372c91a96",
   "metadata": {},
   "source": [
    "### Training & Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b68645d-5a15-41fa-8abd-24531b92fff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate():\n",
    "    \"\"\"\n",
    "    Hard‑routing evaluation: router CE + weighted expert x‑entropy.\n",
    "    No scheduler step inside this function.\n",
    "    \"\"\"\n",
    "    backbone.eval()\n",
    "    router.eval()\n",
    "    total_val = 0.0\n",
    "    batch_sz  = val_loader.batch_size\n",
    "\n",
    "    for batch in tqdm(val_loader, leave=False):\n",
    "        # ---- router part -------------------------------------------------\n",
    "        emb    = get_router_inputs(batch)\n",
    "        logits = router(emb)\n",
    "        r_loss = ce_router(logits, batch[\"expert_label\"].to(device))\n",
    "\n",
    "        # ---- expert part -------------------------------------------------\n",
    "        gate_idx     = logits.argmax(dim=-1)       # (B,)\n",
    "        exp_loss_acc = 0.0\n",
    "\n",
    "        for i, expert in enumerate(expert_names):\n",
    "            sel = (gate_idx == i).nonzero(as_tuple=True)[0]\n",
    "            if sel.numel() == 0:\n",
    "                continue\n",
    "            backbone.set_adapter(expert)\n",
    "\n",
    "            msgs = [batch[\"messages\"][j] for j in sel.tolist()]\n",
    "\n",
    "            # a) text prompt\n",
    "            chat_texts = [\n",
    "                processor.apply_chat_template(m, tokenize=False, add_generation_prompt=False)\n",
    "                for m in msgs\n",
    "            ]\n",
    "            # b) vision tensors\n",
    "            image_inputs, video_inputs = process_vision_info(msgs)\n",
    "            inputs = processor(\n",
    "                text   = chat_texts,\n",
    "                images = image_inputs,\n",
    "                videos = video_inputs,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(device)\n",
    "\n",
    "            # c) labels\n",
    "            lbl = []\n",
    "            for ids in inputs.input_ids.tolist():\n",
    "                msk = [-100]*len(ids)\n",
    "                for s,e in find_assistant_content_sublist_indexes(ids):\n",
    "                    msk[s:e] = ids[s:e]\n",
    "                lbl.append(msk)\n",
    "            label_ids = torch.tensor(lbl, dtype=torch.long, device=device)\n",
    "\n",
    "            out = backbone(**inputs, labels=label_ids)\n",
    "\n",
    "            # weight by share of samples this expert handled\n",
    "            exp_loss_acc += (sel.numel() / batch_sz) * out.loss\n",
    "\n",
    "        total_val += (r_loss + exp_loss_acc).item()\n",
    "\n",
    "    return total_val / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c47b071-70ff-4606-a426-9bc2ba14b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch():\n",
    "    \"\"\"\n",
    "    One epoch of joint router‑adapter training with:\n",
    "      • hard routing\n",
    "      • per‑sample weighted expert loss\n",
    "      • gradient clipping\n",
    "    \"\"\"\n",
    "    backbone.train()\n",
    "    router.train()\n",
    "    total_loss = 0.0\n",
    "    batch_sz   = train_loader.batch_size\n",
    "\n",
    "    for step, batch in enumerate(train_loader, 1):\n",
    "        # 1) Router forward\n",
    "        emb     = get_router_inputs(batch)                 # (B,768)\n",
    "        logits  = router(emb)                              # (B,4)\n",
    "        r_loss  = ce_router(logits, batch[\"expert_label\"].to(device))\n",
    "\n",
    "        # 2) Expert forward (hard routing)\n",
    "        gate_idx     = logits.argmax(dim=-1)               # (B,)\n",
    "        exp_loss_acc = 0.0       # weighted sum over experts\n",
    "        seen_samples = 0         # how many samples contributed\n",
    "\n",
    "        for i, expert in enumerate(expert_names):\n",
    "            sel = (gate_idx == i).nonzero(as_tuple=True)[0]\n",
    "            if sel.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            backbone.set_adapter(expert)\n",
    "            seen_samples += sel.numel()\n",
    "\n",
    "            group_msgs = [batch[\"messages\"][j] for j in sel.tolist()]\n",
    "\n",
    "            # 2a) build chat text (teacher forcing)\n",
    "            chat_texts = [\n",
    "                processor.apply_chat_template(\n",
    "                    m, tokenize=False, add_generation_prompt=False\n",
    "                )\n",
    "                for m in group_msgs\n",
    "            ]\n",
    "            # 2b) vision‑to‑tensor\n",
    "            image_inputs, video_inputs = process_vision_info(group_msgs)\n",
    "            inputs = processor(\n",
    "                text   = chat_texts,\n",
    "                images = image_inputs,\n",
    "                videos = video_inputs,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(device)\n",
    "\n",
    "            # 2c) build labels\n",
    "            lbl = []\n",
    "            for ids in inputs.input_ids.tolist():\n",
    "                msk = [-100] * len(ids)\n",
    "                for s, e in find_assistant_content_sublist_indexes(ids):\n",
    "                    msk[s:e] = ids[s:e]\n",
    "                lbl.append(msk)\n",
    "            label_ids = torch.tensor(lbl, dtype=torch.long, device=device)\n",
    "\n",
    "            # 2d) forward & accumulate *scaled* by (#samples / batch_size)\n",
    "            out  = backbone(**inputs, labels=label_ids)\n",
    "            exp_loss_acc += (sel.numel() / batch_sz) * out.loss\n",
    "\n",
    "        # 3) Total loss = router + weighted expert\n",
    "        loss = r_loss + exp_loss_acc\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(adapter_params + list(router.parameters()), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # ---- logging every 100 steps\n",
    "        if step % 100 == 0:\n",
    "            print(f\"[step {step:4d}] router={r_loss.item():.4f}  \"\n",
    "                  f\"expert={exp_loss_acc.item():.4f}\")\n",
    "\n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b0841-565e-4778-ae91-e42ee239e73a",
   "metadata": {},
   "source": [
    "### Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5361a4-bfaa-4e89-a8ce-b35433667ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f507b13b-53e5-422f-a23a-03cd9a269df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "OUTPUT_DIR   = \"/home/aritrad/moe-directory/moe-datasets/TDIUC/custom-moe/using-sbert/moe-end2end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd089ad9-ffdb-47c6-9fdc-ad9577c9cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val = float(\"inf\")\n",
    "\n",
    "for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "    \n",
    "    tr_loss  = train_epoch()\n",
    "    val_loss = validate()\n",
    "    scheduler_step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch}  train_loss={tr_loss:.4f}  val_loss={val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "\n",
    "        ckpt_root = os.path.join(OUTPUT_DIR, \"best-adapters-2.5-7B\")\n",
    "        os.makedirs(ckpt_root, exist_ok=True)\n",
    "\n",
    "        for name in expert_names:\n",
    "\n",
    "            # Activate that particular adapter before saving.\n",
    "            backbone.set_adapter(name)                             \n",
    "            \n",
    "            out_dir = os.path.join(ckpt_root, name)\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "            # Save only the selected adapter in the loop currently.\n",
    "            backbone.save_pretrained(out_dir, adapter_name=name)\n",
    "            \n",
    "        router_path = os.path.join(OUTPUT_DIR, \"best-router-2.5-7B.pt\")\n",
    "        torch.save(router.state_dict(), router_path)\n",
    "        print(f\" ↳ Saved checkpoint @ epoch {epoch}, val_loss={best_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8e8a62-1bbb-4040-a04f-c98f193001ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c010e6bd-a32d-4c2d-b1d0-26dd60111d52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d379bc-9b8b-40d6-9062-c588b97bb146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f162fee6-9771-478d-b38e-32ab7f314c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec25440-86ef-49f5-8225-9b6662b9c29e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
