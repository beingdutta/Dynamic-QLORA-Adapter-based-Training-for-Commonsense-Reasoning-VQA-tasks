{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c5c54c-342b-484b-9ce0-e1263132517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437a5205-ae83-4f84-8759-b4d9c5943995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, AutoConfig, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02449ad-4d7e-469a-8b90-81ebeab4e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.vision_util import process_vision_info\n",
    "from util.logutil import init_logger, get_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9ad283-f10c-4f7b-8fbc-8af07ef37d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda_env = os.environ.get(\"CONDA_DEFAULT_ENV\")\n",
    "print(f\"Current Conda environment: {conda_env}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1fb12c-63b7-42b4-80e9-afe8e3ac49f2",
   "metadata": {},
   "source": [
    "### Load the Prototype Test Mixed Precision Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb86424d-4e39-4e07-b385-34f5fef2b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ceb201-401e-4506-b875-37ccf3b6f40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpickling (De -serialization)\n",
    "\n",
    "with open('/home/aritrad/MOE-Directory/moe-datasets/TDIUC/custom-moe/Test-Set/prototype-test-set-1.5K-machine-automatic-llama3.2-annotation.pickle', 'rb') as file:\n",
    "    test_set_manual_annt = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6dfaf2-1bf9-4746-afe0-94dde6078f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_manual_annt[0:2], len(test_set_manual_annt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9749d10b-b289-43ed-a639-4e03e5a2b412",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Lenght of mixed reasoing dataset: {len(test_set_manual_annt)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab699eb0-e9b8-47ac-bfbb-b92bb8a2b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d888c0c2-67ce-4397-9b8a-5933061c8020",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder_path = \"/home/aritrad/MOE-Directory/moe-datasets/TDIUC/TDIUC/Images/val2014\"\n",
    "prefix = \"Generate a one word answer for the given image and question: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce613199-5514-48c2-9194-7d993f2b24c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_names = [\"Physical Reasoning.\", \"Quantity Reasoning.\", \"Spatial Reasoning.\", \"Social and Emotional Reasoning.\"]\n",
    "label2id = {name: idx for idx, name in enumerate(expert_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fab794c-be99-4fe4-9ece-09cc53d82ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using list comprehension to update reasoning_type\n",
    "test_set_manual_annt = [\n",
    "    {**item, 'reasoning_type': label2id[item['reasoning_type']]}\n",
    "    for item in test_set_manual_annt\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70361ac2-d101-4d49-8597-d72194f3abea",
   "metadata": {},
   "outputs": [],
   "source": [
    "listToDictionary = {\n",
    "    'question': [ prefix + dict_['question'] for dict_ in test_set_manual_annt ], \n",
    "    'image': [ os.path.join(image_folder_path, dict_['image_id']) for dict_ in test_set_manual_annt ],\n",
    "    'answer': [ dict_['answer'] for dict_ in test_set_manual_annt ], \n",
    "}\n",
    "\n",
    "test_set = Dataset.from_dict(listToDictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae414241-070d-4f5f-a2ae-43ddafd418d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db71b422-bbe0-4950-a44f-73ad847faed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3aceb55-6599-40be-b3f7-e8c3164c6df0",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881e24d5-a7ef-4769-bbe8-e9b99207334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3532954-624a-4d98-9db0-a529afb1e970",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fce157-bcf6-4c4f-832e-5a77e26bb955",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    attn_implementation = \"flash_attention_2\",\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64973c0b-39d8-41ac-9de2-3c72877426be",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in backbone.parameters())\n",
    "trainable_params = sum(p.numel() for p in backbone.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009aa4f8-0d3e-42ca-a018-dfc28ac1aa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processor. \n",
    "# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=256*28*28, max_pixels=512*28*28, padding_side=\"left\", use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e8e442-56fd-4c59-a371-44c03aa63031",
   "metadata": {},
   "source": [
    "### Fetching Expert Names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833cb24f-9ba8-4c2d-a1eb-2ae95e3064e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5f47b3-e8e6-4aa2-828b-63c2020dd159",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_names = []\n",
    "\n",
    "# Hard code the order of loading the experts as in the Training Data.\n",
    "inference_adapter_names = [\n",
    "    \"physical\",      # trains on \"Physical Reasoning.\"\n",
    "    \"quantitative\",  # trains on \"Quantity Reasoning.\"\n",
    "    \"spatial\",       # trains on \"Spatial Reasoning.\"\n",
    "    \"social\"         # trains on \"Social and Emotional Reasoning.\"\n",
    "]\n",
    "\n",
    "ADAPTER_ROOT = '/home/aritrad/MOE-Directory/moe-datasets/TDIUC/custom-moe/moe-end2end/2-2B/best_adapters'\n",
    "\n",
    "for name in inference_adapter_names:\n",
    "    path = Path(ADAPTER_ROOT) / f\"{name}\" \n",
    "    backbone.load_adapter(path, adapter_name=name, is_trainable=False)\n",
    "    expert_names.append(name)\n",
    "    \n",
    "print(\"Experts:\", expert_names)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6246d0b4-7b8a-4d3b-966b-e68936a447c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57120cd6-79b6-457b-bad9-e8d08b91672f",
   "metadata": {},
   "source": [
    "### Load the Trained Router:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f620b36-9985-4f22-b9e0-6d6fcc73da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Router(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden=768, n_experts=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden // 2, hidden // 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden // 4, n_experts)\n",
    "        )\n",
    "        \n",
    "    def forward(self, fused):          # (B, 1536)\n",
    "        return self.net(fused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0d7e02-a08e-4a0a-a566-ae6f4a340798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for testing saved router checkpoint on other test sets.\n",
    "router = Router(n_experts=len(expert_names)).to(device)\n",
    "\n",
    "# Comment this block when Traininig.\n",
    "checkpoint = torch.load( '/home/aritrad/MOE-Directory/moe-datasets/TDIUC/custom-moe/Checkpoints/best_router.pt') \n",
    "router.load_state_dict(checkpoint)\n",
    "\n",
    "print(\"Router Initialized ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d826d6-3799-4b91-870f-a2210c48bb3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a831e71-ef7b-44a7-8426-100b5e838f1b",
   "metadata": {},
   "source": [
    "### Contextual Text Embedding From SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af03e30-126f-4de5-8ec9-a9bd53020115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "sbert = SentenceTransformer('all-mpnet-base-v2', device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc2d33e-ad22-4725-b6a0-e4ec842aa655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa294c32-878e-4fac-8dcf-8d0a37ab9e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_repr(batch):\n",
    "    \"\"\"\n",
    "    batch is a dict with\n",
    "      - batch[\"question\"]: List[str]\n",
    "      - batch[\"labels\"]:   Tensor\n",
    "    returns a torch.Tensor of shape (B, 768) on `device`\n",
    "    \"\"\"    \n",
    "    # SBERT.encode by default runs under no_grad, so SBERT stays frozen.\n",
    "    embeds = sbert.encode(\n",
    "        batch[\"question\"],\n",
    "        convert_to_tensor=True,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # dtype=torch.float32\n",
    "    return embeds  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10899658-81d2-4339-9d4d-ef02728ffff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77cd9e64-cdcd-4b8a-ab42-035fe1c63ffa",
   "metadata": {},
   "source": [
    "### Choose Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a053f2b-67c3-4384-b374-aa6022061ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def choose_expert(batch):\n",
    "\n",
    "    # Get embedding\n",
    "    sent_vec = get_text_repr(batch)     \n",
    "    \n",
    "    # Pick the highest‑scoring expert for each sample\n",
    "    # Tensor of shape (B,)\n",
    "    return router(sent_vec).argmax(dim=-1)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5b8979-f34e-43e2-80c0-d9abc1d1f84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Unit Test: Test the choose_expert function.\n",
    "\n",
    "sample_batch = next(iter(test_loader))\n",
    "print(choose_expert(sample_batch))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4e2f34-0fad-4b90-9770-24eea6e93bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eb8380-5319-41ed-ab85-abbaabe8e36a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad260ede-3568-472a-accf-5b545515fb3d",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6bdb63-2b8d-4a9e-973e-db94c251030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_answer(batch, expert_idx):\n",
    "    \"\"\"\n",
    "    batch        : dict from collate_fn with keys \"image\" (list[str]) and \"question\" (list[str])\n",
    "    expert_idx   : Tensor of shape (B,), each entry in [0..3]\n",
    "\n",
    "    Returns: list of (adapter_name, answer) length B\n",
    "    \"\"\"\n",
    "    # print(f\"Question: {batch['question']}\")\n",
    "    \n",
    "    answers = []\n",
    "    for j, idx in enumerate(expert_idx.tolist()):\n",
    "        adapter = expert_names[idx]\n",
    "\n",
    "        # switch to this expert (activate the adapter)\n",
    "        backbone.set_adapter(adapter)   \n",
    "\n",
    "        # 1) build a single‐sample chat message\n",
    "        message = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": batch[\"image\"][j]},\n",
    "                    {\"type\": \"text\",  \"text\": batch[\"question\"][j] + \" ?\"},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # 2) apply the chat template (adds system prompt, generation prompt)\n",
    "        text_input = processor.apply_chat_template(\n",
    "            message, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # 3) extract vision inputs (uses your existing util)\n",
    "        image_inputs, video_inputs = process_vision_info(message)\n",
    "\n",
    "        # 4) pack everything into model tensors\n",
    "        inputs = processor(\n",
    "            text   = text_input,\n",
    "            images = image_inputs,\n",
    "            videos = video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "\n",
    "        # 5) generate with this expert\n",
    "        generated_ids = backbone.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "        )\n",
    "\n",
    "        # 6) trim off the prompt tokens and decode\n",
    "        in_len   = inputs.input_ids.shape[1]\n",
    "        out_ids  = generated_ids[0, in_len:]\n",
    "        answer   = processor.batch_decode(\n",
    "            [out_ids],\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=False,\n",
    "        )[0].strip()\n",
    "\n",
    "        answers.append((adapter, answer))\n",
    "        \n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74caa7b-2f6a-4328-ba78-a81fb4876bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdd40817-3a30-4525-9380-fa56de58634d",
   "metadata": {},
   "source": [
    "### Test-Set Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d9f927-c87e-4ff3-a5d3-bc72514dafd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    \"\"\"\n",
    "    examples: list of dicts, each with keys\n",
    "      - \"image\"    : str (filepath)\n",
    "      - \"question\" : str\n",
    "\n",
    "    Returns a batch dict with two lists:\n",
    "      - batch[\"image\"]    : list[str]\n",
    "      - batch[\"question\"] : list[str]\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"image\":    [ex[\"image\"]    for ex in examples],\n",
    "        \"question\": [ex[\"question\"] for ex in examples],\n",
    "        \"answer\": [ex[\"answer\"] for ex in examples]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68de5252-1a37-4e60-8fb7-e009186e0645",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_set,\n",
    "                          batch_size = 16,\n",
    "                          shuffle = False,\n",
    "                          collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cf62e2-4332-4817-ac8e-4b4df520858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the batch.\n",
    "\n",
    "for batch in test_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3629b404-0717-4004-84d1-b372dce67373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "723c1f78-a6e4-44f6-8a0e-335c7e28d70b",
   "metadata": {},
   "source": [
    "## Final Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c550735b-0f0e-49ce-9b31-f87cf05de46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_JSONL = \"/home/aritrad/MOE-Directory/moe-datasets/TDIUC/custom-moe/JSON-Reports/end-to-end-trained-architecture-predictions-report.jsonl\"\n",
    "chosen_experts, answers = [],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a45bd6-da24-4a82-883c-51ce11a1816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(OUTPUT_JSONL).open(\"w\") as fout:\n",
    "    \n",
    "    for batch in tqdm(test_loader):\n",
    "        # ❶ run your router\n",
    "        expert_idx = choose_expert(batch)             # returns Tensor of size (B,)\n",
    "\n",
    "        # ❷ generate with the chosen expert per sample\n",
    "        preds = generate_answer(batch, expert_idx)    # list of (adapter, answer)\n",
    "\n",
    "        # print(f'Answer: {preds}\\n')\n",
    "\n",
    "        # Separate and accumulate\n",
    "        adapters, ans = zip(*preds)\n",
    "        chosen_experts.extend(adapters)\n",
    "        answers.extend(ans)\n",
    "\n",
    "        # ❸ write out each line\n",
    "        for img_path, question, groundTruthAnswer, (adapter, answer) in zip(\n",
    "                batch[\"image\"],\n",
    "                batch[\"question\"],\n",
    "                batch[\"answer\"],\n",
    "                preds\n",
    "        ):\n",
    "            fout.write(json.dumps({\n",
    "                #\"image\"        : img_path,\n",
    "                \"question\"     : question.split(': ')[1],\n",
    "                \"chosenExpert\" : adapter,\n",
    "                \"groundTruth\"  : groundTruthAnswer,\n",
    "                \"answer\"       : answer,\n",
    "            }) + \"\\n\")\n",
    "\n",
    "print(f\"✓ Done. Predictions saved to {OUTPUT_JSONL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e4f42d-66e8-4555-ad85-df9c4b91de85",
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth_answer = test_set['answer']\n",
    "generated_answer = answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3f99de-03ba-4879-87ca-26f13621e705",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c9a97-0d77-4901-9d39-c49281952f9a",
   "metadata": {},
   "source": [
    "### Calculate Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4eafde-2ac6-4b1a-8cf8-40f1f606669b",
   "metadata": {},
   "source": [
    "### Evaluate Exact String Match (EM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b2a65b-bc2e-49e7-9949-bcf9826d22b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for accuracy calculation\n",
    "correct_predictions = 0\n",
    "total_predictions = len(generated_answer)\n",
    "\n",
    "# Loop through the results and compare answers\n",
    "for i in range(len(generated_answer)):\n",
    "    if generated_answer[i].strip().lower() == groundtruth_answer[i].strip().lower():\n",
    "        correct_predictions += 1\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (correct_predictions / total_predictions) * 100\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d256f052-8093-4145-97d4-63586f73c860",
   "metadata": {},
   "source": [
    "### BERT Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a27de89-28e7-4744-a994-408e8b71d4d2",
   "metadata": {},
   "source": [
    "Evaluating with BERT Score:\n",
    "\n",
    "Precision (P): How much of the candidate's content is relevant.\n",
    "\n",
    "Recall (R): How much of the reference's content is covered by the candidate.\n",
    "\n",
    "F1 Score (F1): Harmonic mean of Precision and Recall, commonly used as the final metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4482730d-9599-4419-9e44-954c0bc60104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# Example references and candidates\n",
    "# references = ['stool','no','person','stool','sign','bronze','door','no','red','chair','red','black']\n",
    "# candidates = ['stool','no','child','stool','sign','gold','picture','no','brown','chair','brown','black']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b293f2-3502-487b-ada6-5e8cdecbf79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute BERTScore, answerList_test = ground truth, result_list = model generated.\n",
    "\n",
    "P, R, F1 = score(generated_answer, groundtruth_answer, lang=\"en\", verbose=True, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86147c09-18c2-41cb-b828-ca385c8f8bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print scores\n",
    "print(\"Mean Precision:\", np.round(np.mean(P.tolist()) * 100, 2) )\n",
    "print(\"Mean Recall:\", np.round(np.mean(R.tolist()) * 100, 2) )\n",
    "print(\"Mean F1 Score:\", np.round(np.mean(F1.tolist()) * 100, 2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bfd182-d4af-4d5e-a6d4-d491164b25b3",
   "metadata": {},
   "source": [
    "### Evaluating with BLEU-1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae40065-33ad-40e9-9057-25bd25a17e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaf0935-d4d0-4d31-99cd-ea5ae6529df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_scores = []\n",
    "\n",
    "# Function to compute BLEU-1 score for a list of ground truth and predicted answers\n",
    "def calculate_bleu_1_score(ground_truth, predicted):\n",
    "\n",
    "    # This sets BLEU-1 to only consider unigram precision\n",
    "    weights = [1.0] + [0.0] * 3  \n",
    "    \n",
    "    # Smoothing function to handle cases with no n-gram matches\n",
    "    smoothing_function = SmoothingFunction().method1  \n",
    "    \n",
    "    for gt, pred in zip(ground_truth, predicted):\n",
    "        score = sentence_bleu([gt], pred, weights=weights, smoothing_function=smoothing_function)  \n",
    "        bleu_scores.append(score)\n",
    "    \n",
    "    avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "    \n",
    "    return avg_bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b4e714-8e78-4cfe-bd2d-a303cd25268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the BLEU score\n",
    "\n",
    "avg_bleu = calculate_bleu_1_score(groundtruth_answer, generated_answer)\n",
    "print(f\"Average BLEU score: {np.round(avg_bleu*100, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdd6e74-e72a-4d66-8d22-92abe0af6a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c584a889-2df4-4f77-8b26-91727b06acfd",
   "metadata": {},
   "source": [
    "### ROUGE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741abbf2-b13e-4c0e-9396-d43f9b89ed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aca819-73c9-490b-8e53-e7c1fc546b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_rouge_scores(references, candidates):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    rouge1_scores, rouge2_scores, rougeL_scores = [], [], []\n",
    "\n",
    "    for ref, cand in zip(references, candidates):\n",
    "        scores = scorer.score(ref, cand)\n",
    "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "    return {\n",
    "        \"ROUGE-1\": np.round(np.mean(rouge1_scores), 4),\n",
    "        \"ROUGE-2\": np.round(np.mean(rouge2_scores), 4),\n",
    "        \"ROUGE-L\": np.round(np.mean(rougeL_scores), 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94747bc8-de01-4ee3-a0b1-3003231bb404",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rouge = calculate_avg_rouge_scores(groundtruth_answer, generated_answer)\n",
    "avg_rouge = { k:round(v*100, 2) for k,v in avg_rouge.items()}\n",
    "print(\"Average ROUGE scores:\", avg_rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b729265-f3df-4301-8c6d-2ae9d96f6fde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1d82ebf-003f-4d54-9795-73aec3687e77",
   "metadata": {},
   "source": [
    "## Cosine Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e96422-3af1-4084-a2fa-a01a97dfb4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert = SentenceTransformer('all-mpnet-base-v2', device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ce3def-dcab-449e-86e5-7959b4bfa2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findCosSim(word1:str, word2:str) -> int:\n",
    "\n",
    "    # Compute the embeddings\n",
    "    embedding1 = sbert.encode(word1, convert_to_tensor=True)\n",
    "    embedding2 = sbert.encode(word2, convert_to_tensor=True)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    cosine_score = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "    return round(cosine_score.item(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c60144c-4e53-4ada-8ffe-94755f7888e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosineAccuracy = [ findCosSim( generated_answer[idx], groundtruth_answer[idx] ) > 0.71 for idx in tqdm(range(len(generated_answer))) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b4dd22-cf33-4129-8ad3-a71c14b6a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "( sum(cosineAccuracy) / len(cosineAccuracy) ) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc51940-4c8e-41c8-88cd-1db5c77d9fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fe9b26-b408-4fef-9f59-73faa2aedb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d6544a-da4b-4d6d-9839-320e919e63f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc59c58-7a11-4c0e-be72-2e6e464ee0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
